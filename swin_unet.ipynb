{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnpuI4bXVZPY"
      },
      "outputs": [],
      "source": [
        "## Import dependencies\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.figure_factory as ff\n",
        "import plotly.offline as offline\n",
        "\n",
        "import os\n",
        "import pathlib\n",
        "import gc\n",
        "import sys\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import datetime as dt\n",
        "from tqdm import tqdm\n",
        "from pprint import pprint\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.transforms.functional as TF\n",
        "from torchvision.models import resnet18\n",
        "!pip install torchinfo -q --user\n",
        "from torchinfo import summary\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "print('import done!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## For reproducible results\n",
        "def seed_all(s):\n",
        "    random.seed(s)\n",
        "    np.random.seed(s)\n",
        "    torch.manual_seed(s)\n",
        "    torch.cuda.manual_seed(s)\n",
        "    os.environ['PYTHONHASHSEED'] = str(s)\n",
        "    print('Seeds setted!')\n",
        "\n",
        "global_seed = 42\n",
        "seed_all(global_seed)"
      ],
      "metadata": {
        "id": "LK6SfTQUVcZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Data Loading\n",
        "data_config = {'train_csv_path': '../input/uw-madison-gi-tract-image-segmentation/train.csv',\n",
        "               'train_folder_path': '../input/uw-madison-gi-tract-image-segmentation/train',\n",
        "               'test_folder_path': '../input/uw-madison-gi-tract-image-segmentation/test',\n",
        "               'sample_submission_path': '../input/uw-madison-gi-tract-image-segmentation/sample_submission.csv',\n",
        "              }\n",
        "\n",
        "train_df = pd.read_csv(data_config['train_csv_path'])\n",
        "submission_df = pd.read_csv(data_config['sample_submission_path'])\n",
        "\n",
        "print(f'train_length: {len(train_df)}')\n",
        "print(f'submission_length: {len(submission_df)}')"
      ],
      "metadata": {
        "id": "bpIimSWaVht9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Null Value Check\n",
        "print('train_df.info()'); print(train_df.info(), '\\n')\n",
        "\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "f9YiYRGvVmRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Separate 'id' columns' texts, and create new id columns.\n",
        "## This code takes about 2 minutets to execute.\n",
        "\n",
        "def create_id_list(text, p_train = pathlib.Path(data_config['train_folder_path'])):\n",
        "    t = text.split('_')\n",
        "\n",
        "    case_id = t[0][4:]\n",
        "    day_id = t[1][3:]\n",
        "    slice_id = t[3]\n",
        "\n",
        "    case_folder = t[0]\n",
        "    day_folder = ('_').join([t[0], t[1]])\n",
        "    slice_file = ('_').join([t[2], t[3]])\n",
        "\n",
        "    p_folder = p_train / case_folder / day_folder / 'scans'\n",
        "    file_name = [p.name for p in p_folder.iterdir() if p.name[6:10] == slice_id]\n",
        "    id_list = [case_id, day_id, slice_id, case_folder, day_folder, slice_file]\n",
        "    id_list.extend(file_name)\n",
        "    return id_list\n",
        "\n",
        "def create_new_ids(dataframe, new_ids = ['case_id', 'day_id', 'slice_id', 'case_folder', 'day_folder', 'slice_file', 'file_name']):\n",
        "    dataframe['id_list'] = dataframe['id'].map(create_id_list)\n",
        "    for i, item in enumerate(new_ids):\n",
        "        dataframe[item] = dataframe['id_list'].map(lambda x: x[i])\n",
        "    dataframe = dataframe.drop(['id_list'], axis=1)\n",
        "    return dataframe\n",
        "\n",
        "train_df = create_new_ids(train_df)\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "w5qKrneRVpeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Create detection column (1: non NaN segmentation, 0: NaN segmentation).\n",
        "train_df['detection'] = train_df['segmentation'].notna() * 1\n",
        "train_df.head()"
      ],
      "metadata": {
        "id": "26rN0VR4Vuw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_img_n = int(len(train_df) / 3)\n",
        "print('The number of imgs: ', total_img_n)"
      ],
      "metadata": {
        "id": "t3Zpx7asVxUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Calculate segmentation areas and img size.\n",
        "def cal_pos_area(segmentation):\n",
        "    pos_area = 0\n",
        "    if type(segmentation) is str:\n",
        "        seg_list = segmentation.split(' ')\n",
        "        for i in range(len(seg_list)//2):\n",
        "            pos_area += int(seg_list[i*2 + 1])\n",
        "    return pos_area\n",
        "\n",
        "def cal_total_area(file_name):\n",
        "    img_h = int(file_name[11:14])\n",
        "    img_w = int(file_name[15:18])\n",
        "    total_area = img_h * img_w\n",
        "    return total_area\n",
        "\n",
        "train_df['pos_area'] = train_df['segmentation'].map(cal_pos_area)\n",
        "train_df['total_area'] = train_df['file_name'].map(cal_total_area)\n",
        "train_df['pos_area_percentage'] = train_df['pos_area'] / train_df['total_area'] * 100\n",
        "\n",
        "## Check\n",
        "train_df[1920:1930]"
      ],
      "metadata": {
        "id": "2Hv5tN9FV4fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split the samples based on the 'class'.\n",
        "train_lb_df = train_df[train_df['class']=='large_bowel'].reset_index(drop=True)\n",
        "train_sb_df = train_df[train_df['class']=='small_bowel'].reset_index(drop=True)\n",
        "train_st_df = train_df[train_df['class']=='stomach'].reset_index(drop=True)\n",
        "\n",
        "## Calculate each segmentation pixels' ratio to the total img pixels.\n",
        "lb_area_ratio = train_lb_df['pos_area'].sum() / train_lb_df['total_area'].sum()\n",
        "sb_area_ratio = train_sb_df['pos_area'].sum() / train_sb_df['total_area'].sum()\n",
        "st_area_ratio = train_st_df['pos_area'].sum() / train_st_df['total_area'].sum()\n",
        "bg_area_ratio = 1 - (lb_area_ratio + sb_area_ratio + st_area_ratio)\n",
        "\n",
        "print(lb_area_ratio, sb_area_ratio, st_area_ratio, bg_area_ratio)"
      ],
      "metadata": {
        "id": "XSauk1vBV79U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Split the samples which have non-null values in 'segmentation' as positive ones.\n",
        "train_positive_df = train_df.dropna(subset=['segmentation']).reset_index(drop=True)\n",
        "train_negative_df = train_df[train_df['segmentation'].isna()].reset_index(drop=True)\n",
        "\n",
        "pos_lb_df = train_positive_df[train_positive_df['class']=='large_bowel'].reset_index(drop=True)\n",
        "pos_sb_df = train_positive_df[train_positive_df['class']=='small_bowel'].reset_index(drop=True)\n",
        "pos_st_df = train_positive_df[train_positive_df['class']=='stomach'].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Qw1GtQHcV-nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the bar graph of the detection percentages (per total number of images) of each classes.\n",
        "class_group = train_df.groupby(['class'])['detection'].mean() * 100\n",
        "\n",
        "fig = px.bar(class_group)\n",
        "fig.update_layout(title = \"<span style='font-size:36px;>Detection Percentages (per total number of images) of Each Classes</span>\",\n",
        "                  yaxis_title = 'detection percentage')"
      ],
      "metadata": {
        "id": "Sifc7eMYWEq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the histogram of the detection percentage of large_bowel class in each case_ids\n",
        "lb_detection_mean = train_lb_df.groupby(['case_id'])['detection'].mean() * 100\n",
        "fig = px.histogram(lb_detection_mean, nbins=25, marginal='box')\n",
        "fig.update_layout(title = \"<span style='font-size:36px;>Detection Percentage of 'large_bowel' in Each Case_ids</span>\",\n",
        "                  xaxis_title = 'detection percentage')"
      ],
      "metadata": {
        "id": "fF5qN5HSWHiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot the histogram of the detection percentage of small_bowel class in each case_ids\n",
        "sb_detection_mean = train_sb_df.groupby(['case_id'])['detection'].mean() * 100\n",
        "fig = px.histogram(sb_detection_mean, nbins=25, marginal='box')\n",
        "fig.update_layout(title = \"<span style='font-size:36px;>Detection Percentage of 'small_bowel' in Each Case_ids</span>\",\n",
        "                  xaxis_title = 'detection percentage')"
      ],
      "metadata": {
        "id": "Sv08C68HWKV0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train - Valid - Test split\n",
        "## I split the train, valid, test data based on the case_id (imgs that have the same case_id are assigned in the same set).\n",
        "\n",
        "train_ratio = 0.85\n",
        "valid_ratio = 0.10\n",
        "test_ratio = 0.05\n",
        "\n",
        "case_ids = train_df['case_id'].unique()\n",
        "idxs = np.random.permutation(range(len(case_ids)))\n",
        "cut_1 = int(train_ratio * len(idxs))\n",
        "cut_2 = int((train_ratio + valid_ratio) * len(idxs))\n",
        "\n",
        "train_case_ids = case_ids[idxs[:cut_1]]\n",
        "valid_case_ids = case_ids[idxs[cut_1:cut_2]]\n",
        "test_case_ids = case_ids[idxs[cut_2:]]\n",
        "\n",
        "train = train_df.query('case_id in @train_case_ids')\n",
        "valid = train_df.query('case_id in @valid_case_ids')\n",
        "test = train_df.query('case_id in @test_case_ids')\n",
        "\n",
        "print(len(train), len(valid), len(test), len(train_df))"
      ],
      "metadata": {
        "id": "nxHVjgjDWQK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_case_folders = train['case_folder'].unique()\n",
        "train_files = []\n",
        "for case_folder in train_case_folders:\n",
        "    p_train = pathlib.Path(data_config['train_folder_path'])\n",
        "    p_folder = p_train / case_folder\n",
        "    tmp_files = list(p_folder.glob('**/scans/*.png'))\n",
        "    train_files.extend(tmp_files)\n",
        "\n",
        "valid_case_folders = valid['case_folder'].unique()\n",
        "valid_files = []\n",
        "for case_folder in valid_case_folders:\n",
        "    p_train = pathlib.Path(data_config['train_folder_path'])\n",
        "    p_folder = p_train / case_folder\n",
        "    tmp_files = list(p_folder.glob('**/scans/*.png'))\n",
        "    valid_files.extend(tmp_files)\n",
        "\n",
        "test_case_folders = test['case_folder'].unique()\n",
        "test_files = []\n",
        "for case_folder in test_case_folders:\n",
        "    p_train = pathlib.Path(data_config['train_folder_path'])\n",
        "    p_folder = p_train / case_folder\n",
        "    tmp_files = list(p_folder.glob('**/scans/*.png'))\n",
        "    test_files.extend(tmp_files)\n",
        "\n",
        "print(len(train_files), len(valid_files), len(test_files))\n"
      ],
      "metadata": {
        "id": "SkUYZ3qjWTje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Building Dataset and DataLoader\n",
        "class UWMadison2022Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, dataframe=None, input_shape=256,):\n",
        "        self.files = files\n",
        "        self.df = dataframe\n",
        "        self.input_shape = input_shape\n",
        "        self.transforms = transforms.Compose([\n",
        "            transforms.CenterCrop(self.input_shape),\n",
        "            transforms.Normalize(mean=[(0.485+0.456+0.406)/3], std=[(0.229+0.224+0.225)/3]),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        p_file = self.files[idx]\n",
        "        #img = torchvision.io.read_image(p_file)\n",
        "        img = np.array(Image.open(p_file))\n",
        "        img_shape = torch.tensor(img.shape)\n",
        "        img = transforms.functional.to_tensor(img) / 255.\n",
        "        img = self.transforms(img)\n",
        "        #img = torch.cat([img, img, img], dim=0)\n",
        "\n",
        "        if self.df is not None:\n",
        "            f_name = str(p_file).split('/')\n",
        "            case_day_id = f_name[5]\n",
        "            slice_id = f_name[7][:10]\n",
        "            f_id = '_'.join([case_day_id, slice_id])\n",
        "            labels_df = self.df.query('id == @f_id')\n",
        "\n",
        "            label = torch.zeros([img_shape[0]*img_shape[1]])\n",
        "            for i, organ in enumerate(['large_bowel', 'small_bowel', 'stomach']):\n",
        "                segmentation = labels_df[labels_df['class'] == organ]['segmentation'].item()\n",
        "                if type(segmentation) is str:\n",
        "                    segmentation = segmentation.split(' ')\n",
        "                    for j in range(len(segmentation)//2):\n",
        "                        start_idx = int(segmentation[j*2])\n",
        "                        span = int(segmentation[j*2 + 1])\n",
        "                        label[start_idx:(start_idx+span)] = (i+1)\n",
        "            label = torch.reshape(label, (img_shape[0], img_shape[1]))\n",
        "            label = transforms.CenterCrop(self.input_shape)(label)\n",
        "            label = torch.nn.functional.one_hot(label.to(torch.int64), num_classes=4)\n",
        "            label = label.permute(2, 0, 1)\n",
        "            return img, label, img_shape\n",
        "\n",
        "        else: return img, img_shape\n",
        "\n",
        "train_ds = UWMadison2022Dataset(train_files, train, input_shape=256)\n",
        "valid_ds = UWMadison2022Dataset(valid_files, valid, input_shape=256)\n",
        "test_ds = UWMadison2022Dataset(test_files, test, input_shape=256)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "## Checking dataset and dataloder\n",
        "print('------ train_dl ------')\n",
        "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
        "tmp = train_dl.__iter__()\n",
        "x, y, shape = tmp.next()\n",
        "print(f\"x : {x.shape}\")\n",
        "print(f\"labels: {y.shape}\")\n",
        "print(f\"img_shapes: {shape.shape}\")\n",
        "print(f\"n_samples: {len(train_ds)}\")\n",
        "print(f\"n_batches: {len(tmp)}\")\n",
        "print()\n",
        "\n",
        "print('------ valid_dl ------')\n",
        "valid_dl = torch.utils.data.DataLoader(valid_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n",
        "tmp = valid_dl.__iter__()\n",
        "x, y, shape = tmp.next()\n",
        "print(f\"x : {x.shape}\")\n",
        "print(f\"labels: {y.shape}\")\n",
        "print(f\"img_shapes: {shape.shape}\")\n",
        "print(f\"n_samples: {len(valid_ds)}\")\n",
        "print(f\"n_batches: {len(tmp)}\")\n",
        "print()\n",
        "\n",
        "print('------ test_dl ------')\n",
        "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)\n",
        "tmp = test_dl.__iter__()\n",
        "x, y, shape = tmp.next()\n",
        "print(f\"x : {x.shape}\")\n",
        "print(f\"labels: {y.shape}\")\n",
        "print(f\"img_shapes: {shape.shape}\")\n",
        "print(f\"n_samples: {len(test_ds)}\")\n",
        "print(f\"n_batches: {len(tmp)}\")\n",
        "print()"
      ],
      "metadata": {
        "id": "iKp8OYAtWYkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "## Limit GPU Memory in TensorFlow\n",
        "## Because TensorFlow, by default, allocates the full amount of available GPU memory when it is launched.\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if len(physical_devices) > 0:\n",
        "    for device in physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(device, True)\n",
        "        print('{} memory growth: {}'.format(device, tf.config.experimental.get_memory_growth(device)))\n",
        "else:\n",
        "    print(\"Not enough GPU hardware devices available\")"
      ],
      "metadata": {
        "id": "N5js6o_MWiMx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-unet-collection -q -U\n",
        "from keras_unet_collection import models, losses\n",
        "\n",
        "tf_model = models.swin_unet_2d((256, 256, 1), filter_num_begin=64,\n",
        "                               n_labels=4, depth=4, stack_num_down=2, stack_num_up=2,\n",
        "                               patch_size=(4, 4), num_heads=[4, 8, 8, 8],\n",
        "                               window_size=[4, 2, 2, 2], num_mlp=512,\n",
        "                               output_activation='Softmax', shift_window=True,\n",
        "                               name='swin_unet')"
      ],
      "metadata": {
        "id": "C2iP2J7nWk9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_model.compile(loss='binary_crossentropy',\n",
        "              optimizer=keras.optimizers.Adam(lr=1e-3),\n",
        "              metrics=['accuracy', losses.dice_coef])\n",
        "tf_model.summary()\n",
        "## To train this tf_model, we have to create TensorFlow Datasets."
      ],
      "metadata": {
        "id": "ORbU0sAXWoDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Focal Loss Function\n",
        "class SegmentationFocalLoss(nn.Module):\n",
        "    def __init__(self, gamma=2, weight=None):\n",
        "        super().__init__()\n",
        "        self.gamma = gamma\n",
        "        if torch.cuda.is_available():\n",
        "            self.loss = torch.nn.CrossEntropyLoss(weight=weight).cuda()\n",
        "        else:\n",
        "            self.loss = nn.CrossEntropyLoss(weight=weight)\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        ce_loss = self.loss(pred, target)\n",
        "        #ce_loss = torch.nn.functional.cross_entropy(pred, target, reduce=False)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (1. - pt) ** self.gamma * ce_loss\n",
        "        return torch.mean(focal_loss)\n",
        "\n",
        "##Setting the weight parameter of CrossEntropyLoss.\n",
        "lb_weight = 1 / lb_area_ratio\n",
        "sb_weight = 1 / sb_area_ratio\n",
        "st_weight = 1 / st_area_ratio\n",
        "bg_weight = 1 / bg_area_ratio\n",
        "total_weight = lb_weight + sb_weight + st_weight + bg_weight\n",
        "\n",
        "lb_weight = lb_weight / total_weight * 5\n",
        "sb_weight = sb_weight / total_weight * 5\n",
        "st_weight = st_weight / total_weight * 5\n",
        "bg_weight = bg_weight / total_weight * 5\n",
        "weight = torch.tensor([bg_weight, lb_weight, sb_weight, st_weight], dtype=torch.float)\n",
        "print(f'bg:{bg_weight}, lb:{lb_weight}, sb:{sb_weight}, st{st_weight}')\n",
        "\n",
        "loss_fn = SegmentationFocalLoss(gamma=3, weight=weight)"
      ],
      "metadata": {
        "id": "3E56oms6Wsei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LEARNING_RATE = 1e-4\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "KGh2FJ7-Wv7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For the model training loop.\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'\n",
        "else: DEVICE = 'cpu'\n",
        "\n",
        "def train_fn(loader, model, optimizer, loss_fn, device=DEVICE):\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    for batch_idx, (data, targets, img_size) in enumerate(loop):\n",
        "        data = data.to(device=device)\n",
        "        targets = targets.to(device=device)\n",
        "\n",
        "        predictions = model(data)\n",
        "        targets = torch.argmax(targets, dim=1)\n",
        "        loss = loss_fn(predictions, targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "        train_loss += loss.detach().cpu().numpy() * BATCH_SIZE\n",
        "\n",
        "    train_loss = train_loss / (BATCH_SIZE * len(train_dl))\n",
        "    return train_loss\n",
        "\n",
        "## For the model validation loop.\n",
        "def valid_fn(loader, model, loss_fn, device=DEVICE):\n",
        "    model.eval()\n",
        "    valid_loss = 0.\n",
        "    loop = tqdm(loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, targets, img_size) in enumerate(loop):\n",
        "            data = data.to(device=device)\n",
        "            targets = targets.to(device=device)\n",
        "\n",
        "            predictions = model(data)\n",
        "            targets = torch.argmax(targets, dim=1)\n",
        "            loss = loss_fn(predictions, targets)\n",
        "            valid_loss += loss * BATCH_SIZE\n",
        "\n",
        "            loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        valid_loss = valid_loss / (BATCH_SIZE * len(valid_dl))\n",
        "    return valid_loss"
      ],
      "metadata": {
        "id": "G8vmHnQmW0Y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## For the train & validation loop.\n",
        "NUM_EPOCHS = 70\n",
        "\n",
        "## DeepLabv3 model\n",
        "model.to(device=DEVICE)\n",
        "\n",
        "best_loss = 100\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print('-------------')\n",
        "    print('Epoch {}/{}'.format(epoch+1, NUM_EPOCHS))\n",
        "    print('-------------')\n",
        "\n",
        "    train_loss = train_fn(train_dl, model, optimizer, loss_fn, DEVICE)\n",
        "    valid_loss = valid_fn(valid_dl, model, loss_fn, DEVICE)\n",
        "\n",
        "    if valid_loss < best_loss:\n",
        "        checkpoint = {\n",
        "            'model': model.state_dict(),\n",
        "            'optimizer': optimizer.state_dict(),\n",
        "        }\n",
        "        torch.save(checkpoint, \"./checkpoint.pth\")\n",
        "        print('best model saved!')\n",
        "        best_loss = valid_loss\n",
        "\n",
        "    print(f'Train Loss: {train_loss},  Valid Loss: {valid_loss}')"
      ],
      "metadata": {
        "id": "nsgI4QMqW4Gd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}